{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#api\n",
    "import praw\n",
    "\n",
    "#pre-processing\n",
    "import nltk \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "# import string\n",
    "#from textblob import TextBlob\n",
    "#from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import gensim\n",
    "from collections import Counter\n",
    "# nltk.download()\n",
    "\n",
    "from NLP_functions import display_topics, prob_df, topic_matrix\n",
    "\n",
    "#analysis\n",
    "from sklearn.decomposition import TruncatedSVD, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLP_functions import display_topics, prob_df, topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.1.4 was released Sunday February 07, 2021.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='oCxykEUclueAuA', client_secret='M8wVQw3nFxMsXHLGxurnTtkN-Mdvtg', user_agent='Crypto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top posts of all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  score      id  \\\n",
      "0    Elon Musk reminds me of a rich billionaire who...  39192  lce6zm   \n",
      "1    CryptoNick is deleting all of his BitConnect v...  26496  7r0ftz   \n",
      "2    Reminder: Robinhood blocked several stocks fro...  26234  lfrslo   \n",
      "3    I will tell you exactly what is going on here,...  20145  7vga1y   \n",
      "4    Robinhood is launching a Crypto Trading app to...  19963  7sx5ze   \n",
      "..                                                 ...    ...     ...   \n",
      "927  Apple Co-Founder Steve Wozniak Outlines the Pa...   1492  9rcezl   \n",
      "928  PwC Hong Kong and PwC Singapore announce joint...   1487  8gy2l6   \n",
      "929                                 We Are All Satoshi   1485  gqt4ze   \n",
      "930             Top 101 Coins Grouped by Usage/Purpose   1487  lgeots   \n",
      "931  Hackers Donate Stolen Bitcoin Worth $20,000 to...   1489  jeowcu   \n",
      "\n",
      "          subreddit                                                url  \\\n",
      "0    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "1    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "2    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "3    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "4    CryptoCurrency  http://blog.robinhood.com/news/2018/1/24/dont-...   \n",
      "..              ...                                                ...   \n",
      "927  CryptoCurrency  https://dailyhodl.com/2018/10/25/apple-co-foun...   \n",
      "928  CryptoCurrency  https://www.pwchk.com/en/press-room/press-rele...   \n",
      "929  CryptoCurrency                https://i.redd.it/bsykace1l2151.gif   \n",
      "930  CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "931  CryptoCurrency  https://cryptopotato.com/hackers-donate-stolen...   \n",
      "\n",
      "     num_comments                                               body  \\\n",
      "0            3628  Anyone kinda disturbed by his behaviour? He cl...   \n",
      "1            3027  We can't let these legendary affiliate scammer...   \n",
      "2            1098  Stop fucking around with these corporate hacks...   \n",
      "3            1431  /edit: Hi /r/all. While I have your attention,...   \n",
      "4            3857                                                      \n",
      "..            ...                                                ...   \n",
      "927           108                                                      \n",
      "928           337                                                      \n",
      "929            90                                                      \n",
      "930           247  I was sort of shocked I couldn't find this dat...   \n",
      "931           155                                                      \n",
      "\n",
      "                   date  \n",
      "0   2021-02-04 12:02:16  \n",
      "1   2018-01-17 11:46:08  \n",
      "2   2021-02-09 01:55:15  \n",
      "3   2018-02-05 17:21:09  \n",
      "4   2018-01-25 16:19:04  \n",
      "..                  ...  \n",
      "927 2018-10-25 17:31:26  \n",
      "928 2018-05-04 09:55:54  \n",
      "929 2020-05-26 08:28:25  \n",
      "930 2021-02-09 22:02:05  \n",
      "931 2020-10-20 12:54:38  \n",
      "\n",
      "[932 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('Cryptocurrency')\n",
    "\n",
    "for post in list(ml_subreddit.top('all', limit=1000)):\n",
    "    date = datetime.datetime.utcfromtimestamp(post.created_utc)\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, date])\n",
    "    \n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'date'])\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top coments from each top post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               comment                date\n",
      "0     If your life savings hinges on a Tweet from a... 2021-02-04 12:02:16\n",
      "1     **Some Important Reminders:**\\n\\n* Even in th... 2018-01-17 11:46:08\n",
      "2     It’s not even real Bitcoin.  You can never wi... 2021-02-09 01:55:15\n",
      "3     [deleted] It's a shame i can vote this up onl... 2018-02-05 17:21:09\n",
      "4     Reminder: all referral links are prohibited i... 2018-01-25 16:19:04\n",
      "..                                                 ...                 ...\n",
      "927   Not a bad supporter to have! Show me your bag... 2018-10-25 17:31:26\n",
      "928   Alibaba working with PWC on a food supply cha... 2018-05-04 09:55:54\n",
      "929   \"I am the real Slim Shady\" lol Except Craig W... 2020-05-26 08:28:25\n",
      "930   Well written! Didnt know there was soo many s... 2021-02-09 22:02:05\n",
      "931   Who is the true Robinhood here? > Bitcoin Wor... 2020-10-20 12:54:38\n",
      "\n",
      "[932 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "top_comments = []\n",
    "for i,d in zip(posts.id, posts.date):\n",
    "    try:\n",
    "        submission = reddit.submission(id=i)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        comments=list([(comment.body) for comment in submission.comments.list()])\n",
    "        comment_string = ''\n",
    "        for x in comments[:20]:\n",
    "            comment_string = comment_string + ' ' + x\n",
    "        top_comments.append([comment_string,d])\n",
    "    except:\n",
    "        comments=None\n",
    "top_comments = pd.DataFrame(top_comments,columns=['comment', 'date'])\n",
    "print(top_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA of Comments from each year to get stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_comments = comment_year.loc['2017', 'Comments'] #concatenate comments column into one string\n",
    "\n",
    "# all_comments = re.sub(r'\\([^)]*\\)', '', all_comments) #remove links\n",
    "# all_comments = re.sub('\\w*\\d\\w*', '', all_comments) #remove numbers\n",
    "# all_comments = re.sub(r'[^\\w\\s]', '', (all_comments))#remove punctuation\n",
    "# all_comments = all_comments.lower() #lowercase\n",
    "# all_comments = all_comments.replace('btc', 'bitcoin') #replace bitcoin acronym \n",
    "\n",
    "# #create stop words list\n",
    "# stop = stopwords.words('english')\n",
    "# stop += [ 'moderator', 'subreddit', 'reddit' , 'question', 'concern', 'please', 'contact', 'im', \n",
    "#         'people', 'gon', 'seems', 'something', 'year', 'like', 'mod']\n",
    "# stop = set(stop)\n",
    "\n",
    "# mwe_tokenizer = MWETokenizer([ ('bull','run'), ('market','cap'), ('pump','dump'),\n",
    "#                              ('stock','market'), ('flaired', 'inaccurately'), ('get', 'rich', 'quick')])\n",
    "\n",
    "# words = mwe_tokenizer.tokenize(word_tokenize(all_comments))  # tokenize words\n",
    "\n",
    "# #lemmatize words\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# words = [(lemmatizer.lemmatize(w)) for w in words]\n",
    "\n",
    "# words = [w for w in words if w not in stop] #remove stop words\n",
    "\n",
    "# #create n grams\n",
    "# n = 2\n",
    "# bigrams = ngrams(words, n)\n",
    "\n",
    "# #count phrases\n",
    "# counter = Counter()\n",
    "# counter += Counter(bigrams)\n",
    "\n",
    "# for word, count in counter.most_common(30):\n",
    "#     print('%20s %i' % (\" \".join(word), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine comments by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    404\n",
       "2020    200\n",
       "2021    125\n",
       "2019    114\n",
       "2017     89\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_comments['year'] = top_comments['date'].dt.year\n",
    "years = ['2017', '2018', '2019', '2020', '2021']\n",
    "top_comments['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_year = pd.DataFrame(index=years, columns=['Comments'])\n",
    "for y in years:\n",
    "    comment_string = ''\n",
    "    for index, row in top_comments.iterrows():\n",
    "        if str(row[2]) == y:\n",
    "            comment_string = comment_string + ' ' + row[0]\n",
    "            \n",
    "    comment_string = re.sub(r'\\([^)]*\\)', '', comment_string) #remove links\n",
    "    comment_string = re.sub('\\w*\\d\\w*', '', comment_string) #remove numbers\n",
    "    comment_string = re.sub(r'[^\\w\\s]', '', (comment_string))#remove punctuation\n",
    "    comment_string = comment_string.lower() #lowercase\n",
    "    comment_string = comment_string.replace('btc', 'bitcoin') #replace bitcoin acronym \n",
    "    comment_string = comment_string.replace('bch', 'bitcoin_cash') \n",
    "    comment_string = comment_string.replace('eth', 'ethereum')\n",
    "    \n",
    "    NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
    "    comment_string = NON_ASCII.sub(r'', comment_string)\n",
    "    \n",
    "    #create stop words list\n",
    "    stop = stopwords.words('english')\n",
    "    stop += [ 'moderator', 'subreddit', 'reddit' , 'question', 'concern', 'please', 'contact', 'im', \n",
    "            'people', 'gon', 'seems', 'something', 'year', 'like', 'mod', 'wa', 'thread', 'crypto', 'get',\n",
    "            'see', 'deleted']\n",
    "    stop = set(stop)\n",
    "\n",
    "    mwe_tokenizer = MWETokenizer([ ('bull','run'), ('market','cap'), ('pump','dump'), ('bitcoin','cash'),\n",
    "                             ('stock','market'), ('flaired', 'inaccurately')])\n",
    "    \n",
    "    comment_token = mwe_tokenizer.tokenize(word_tokenize(comment_string))  # tokenize words\n",
    "    \n",
    "    comment_token = [w.strip() for w in comment_token] #remove spaces\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # 1. Init Lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # 2. Lemmatize Single Word with the appropriate POS tag\n",
    "    word = 'feet'\n",
    "    comment_token_lemm = [lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "    # 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "    sentence = \"The striped bats are hanging on their feet for best\"\n",
    "    print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "    \n",
    "    #lemmatize words\n",
    "#     stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "#     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#     comment_token_lemm = [(stemmer.stem(lemmatizer.lemmatize(w)) for w in comment_token]\n",
    "\n",
    "    comment_token_lemm = [w for w in comment_token_lemm if w not in stop] #remove stop words\n",
    "    \n",
    "    comment_token_lemm = \" \".join(comment_token_lemm) #join back into one string\n",
    "    \n",
    "    comment_year.loc[y, 'Comments'] = comment_token_lemm #add to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comments\n",
      "2017  longer version raccoon loses two piece candyfl...\n",
      "2018  important reminder even case public figure str...\n",
      "2019  twitter post random guy popular giving legitim...\n",
      "2020  offered cabinet position biden admin tldr form...\n",
      "2021  life saving hinge tweet crazy billionaire dont...\n"
     ]
    }
   ],
   "source": [
    "print(comment_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment_year = re.sub(r'\\([^)]*\\)', '', all_comments) #remove links\n",
    "# comment_year = re.sub('\\w*\\d\\w*', '', all_comments) #remove numbers\n",
    "# comment_year = re.sub(r'[^\\w\\s]', '', (all_comments))#remove punctuation\n",
    "# comment_year = comment_year.lower() #lowercase\n",
    "# comment_year = comment_year.replace('btc', 'bitcoin') #replace bitcoin acronym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = s.apply(fun) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(comment_year['Comments'])\n",
    "\n",
    "topics = 2\n",
    "topic_words = 20\n",
    "topic_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa ofensai</th>\n",
       "      <th>aa ofensai de</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaa anonymous</th>\n",
       "      <th>aaa anonymous altcoins</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaaaaah thats</th>\n",
       "      <th>aaaaaah thats whan</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>...</th>\n",
       "      <th>zuckerbucks inevitable</th>\n",
       "      <th>zuckerbucks inevitable control</th>\n",
       "      <th>zuckercash</th>\n",
       "      <th>zuckercash stable</th>\n",
       "      <th>zuckercash stable need</th>\n",
       "      <th>zuckercash zuckercash</th>\n",
       "      <th>zuckercash zuckercash stable</th>\n",
       "      <th>zunes</th>\n",
       "      <th>zunes never</th>\n",
       "      <th>zunes never replace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 423902 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aa ofensai  aa ofensai de  aaa  aaa anonymous  aaa anonymous altcoins  \\\n",
       "0   0           0              0    0              0                       0   \n",
       "1   1           1              1    0              0                       0   \n",
       "2   0           0              0    0              0                       0   \n",
       "3   0           0              0    0              0                       0   \n",
       "4   0           0              0    1              1                       1   \n",
       "\n",
       "   aaaaaah  aaaaaah thats  aaaaaah thats whan  aaaaand  ...  \\\n",
       "0        0              0                   0        0  ...   \n",
       "1        0              0                   0        2  ...   \n",
       "2        0              0                   0        1  ...   \n",
       "3        1              1                   1        0  ...   \n",
       "4        0              0                   0        2  ...   \n",
       "\n",
       "   zuckerbucks inevitable  zuckerbucks inevitable control  zuckercash  \\\n",
       "0                       0                               0           0   \n",
       "1                       0                               0           2   \n",
       "2                       1                               1           0   \n",
       "3                       0                               0           0   \n",
       "4                       0                               0           0   \n",
       "\n",
       "   zuckercash stable  zuckercash stable need  zuckercash zuckercash  \\\n",
       "0                  0                       0                      0   \n",
       "1                  1                       1                      1   \n",
       "2                  0                       0                      0   \n",
       "3                  0                       0                      0   \n",
       "4                  0                       0                      0   \n",
       "\n",
       "   zuckercash zuckercash stable  zunes  zunes never  zunes never replace  \n",
       "0                             0      0            0                    0  \n",
       "1                             1      0            0                    0  \n",
       "2                             0      1            1                    1  \n",
       "3                             0      0            0                    0  \n",
       "4                             0      0            0                    0  \n",
       "\n",
       "[5 rows x 423902 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words=stop, ngram_range=(1,3))\n",
    "X_cv = cv.fit_transform(comments)\n",
    "pd.DataFrame(X_cv.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis (LSA) with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85715558, 0.06249011])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_cv = TruncatedSVD(topics)\n",
    "X_lsa_cv = lsa_cv.fit_transform(X_cv)\n",
    "lsa_cv.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "bitcoin, dont, money, would, one, coin, time, ha, make, good, buy, still, market, know, going, think, thing, even, go, really\n",
      "\n",
      "Topic  1\n",
      "bitcoin, buy, ethereum, doge, time, bought, never, moon, xrp, gif, paypal, buying, defi, pump, alt, sell, meme, sold, always, next\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_cv, cv.get_feature_names(), topic_words, topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_matrix(lsa_cv, cv.get_feature_names(), topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_df(X_lsa_cv, topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF) with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_cv = NMF(topics)\n",
    "X_nmf_cv = nmf_cv.fit_transform(X_cv)\n",
    "# nmf_cv.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "bitcoin, dont, money, coin, would, one, ha, time, make, market, good, still, thing, know, going, think, exchange, even, buy, need\n",
      "\n",
      "Topic  1\n",
      "bitcoin, dont, time, money, buy, one, would, ha, good, coin, make, know, go, think, going, even, still, much, back, day\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_cv, cv.get_feature_names(), topic_words, topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_matrix(nmf_cv, cv.get_feature_names(), topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>5.50617</td>\n",
       "      <td>17.27875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>59.34721</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>5.17410</td>\n",
       "      <td>22.39096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>50.97603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>1.73046</td>\n",
       "      <td>39.91785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_1   topic_2\n",
       "2017   5.50617  17.27875\n",
       "2018  59.34721   0.00000\n",
       "2019   5.17410  22.39096\n",
       "2020   0.00000  50.97603\n",
       "2021   1.73046  39.91785"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df(X_nmf_cv, topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa ofensai</th>\n",
       "      <th>aa ofensai de</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaa anonymous</th>\n",
       "      <th>aaa anonymous altcoins</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaaaaah thats</th>\n",
       "      <th>aaaaaah thats whan</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>...</th>\n",
       "      <th>zuckerbucks inevitable</th>\n",
       "      <th>zuckerbucks inevitable control</th>\n",
       "      <th>zuckercash</th>\n",
       "      <th>zuckercash stable</th>\n",
       "      <th>zuckercash stable need</th>\n",
       "      <th>zuckercash zuckercash</th>\n",
       "      <th>zuckercash zuckercash stable</th>\n",
       "      <th>zunes</th>\n",
       "      <th>zunes never</th>\n",
       "      <th>zunes never replace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.001893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 423902 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aa  aa ofensai  aa ofensai de       aaa  aaa anonymous  \\\n",
       "0  0.000000    0.000000       0.000000  0.000000       0.000000   \n",
       "1  0.000563    0.000563       0.000563  0.000000       0.000000   \n",
       "2  0.000000    0.000000       0.000000  0.000000       0.000000   \n",
       "3  0.000000    0.000000       0.000000  0.000000       0.000000   \n",
       "4  0.000000    0.000000       0.000000  0.001481       0.001481   \n",
       "\n",
       "   aaa anonymous altcoins   aaaaaah  aaaaaah thats  aaaaaah thats whan  \\\n",
       "0                0.000000  0.000000       0.000000            0.000000   \n",
       "1                0.000000  0.000000       0.000000            0.000000   \n",
       "2                0.000000  0.000000       0.000000            0.000000   \n",
       "3                0.000000  0.001303       0.001303            0.001303   \n",
       "4                0.001481  0.000000       0.000000            0.000000   \n",
       "\n",
       "    aaaaand  ...  zuckerbucks inevitable  zuckerbucks inevitable control  \\\n",
       "0  0.000000  ...                0.000000                        0.000000   \n",
       "1  0.000755  ...                0.000000                        0.000000   \n",
       "2  0.001268  ...                0.001893                        0.001893   \n",
       "3  0.000000  ...                0.000000                        0.000000   \n",
       "4  0.001984  ...                0.000000                        0.000000   \n",
       "\n",
       "   zuckercash  zuckercash stable  zuckercash stable need  \\\n",
       "0    0.000000           0.000000                0.000000   \n",
       "1    0.001127           0.000563                0.000563   \n",
       "2    0.000000           0.000000                0.000000   \n",
       "3    0.000000           0.000000                0.000000   \n",
       "4    0.000000           0.000000                0.000000   \n",
       "\n",
       "   zuckercash zuckercash  zuckercash zuckercash stable     zunes  zunes never  \\\n",
       "0               0.000000                      0.000000  0.000000     0.000000   \n",
       "1               0.000563                      0.000563  0.000000     0.000000   \n",
       "2               0.000000                      0.000000  0.001893     0.001893   \n",
       "3               0.000000                      0.000000  0.000000     0.000000   \n",
       "4               0.000000                      0.000000  0.000000     0.000000   \n",
       "\n",
       "   zunes never replace  \n",
       "0             0.000000  \n",
       "1             0.000000  \n",
       "2             0.001893  \n",
       "3             0.000000  \n",
       "4             0.000000  \n",
       "\n",
       "[5 rows x 423902 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(stop_words= stop, ngram_range=(1,3))\n",
    "X_tfid = tfid.fit_transform(comments)\n",
    "pd.DataFrame(X_tfid.toarray(), columns=tfid.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis (LSA) with TF-IDF Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0029449 , 0.33373762])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_tfid = TruncatedSVD(topics)\n",
    "X_lsa_tfid = lsa_tfid.fit_transform(X_tfid)\n",
    "lsa_tfid.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "bitcoin, dont, money, time, would, one, coin, ha, buy, make, good, know, think, going, go, still, even, thing, really, much\n",
      "\n",
      "Topic  1\n",
      "coin, word word, word, price, bitcoincash, word word word, ripple, litecoin, buy, compressed commented, today faq full, full list source, today faq, let know compressed, faq full, compressed commented today, know compressed, faq full list, know compressed commented, full list\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_tfid, tfid.get_feature_names(), topic_words, topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_matrix(lsa_tfid, tfid.get_feature_names(), topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_df(X_lsa_tfid, topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF) with TF-IDF Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_tfid = NMF(topics)\n",
    "X_nmf_tfid = nmf_tfid.fit_transform(X_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "bitcoin, dont, money, time, one, would, ha, buy, make, coin, good, think, know, still, go, going, even, thing, market, really\n",
      "\n",
      "Topic  1\n",
      "bitcoin, coin, would, dont, time, buy, money, ha, one, price, good, much, day, make, going, also, know, really, want, go\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_tfid, tfid.get_feature_names(), topic_words, topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_matrix(nmf_tfid, tfid.get_feature_names(), topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.2536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.60487</td>\n",
       "      <td>0.1656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0.66187</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>0.67885</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>0.66393</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_1  topic_2\n",
       "2017  0.00000   1.2536\n",
       "2018  0.60487   0.1656\n",
       "2019  0.66187   0.0000\n",
       "2020  0.67885   0.0000\n",
       "2021  0.66393   0.0000"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df(X_nmf_tfid, topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = [gensim.utils.simple_preprocess(d) for d in comments]\n",
    "\n",
    "# size refers to the desired dimension of our word vectors\n",
    "# window refers to the size of our context window\n",
    "# sg means that we are using the Skip-gram architecture\n",
    "\n",
    "word2vec = gensim.models.Word2Vec(tokenized_comments, size=10, window=2,min_count=1)\n",
    "\n",
    "# Path to where the word2vec file lives\n",
    "google_vec_file = '/Users/racheldilley/Downloads/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Load it!  This might take a few minutes...\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-292-9a34b3f00e64>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  word2vec.most_similar('doge')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wallet', 0.9982737302780151),\n",
       " ('back', 0.9982404708862305),\n",
       " ('point', 0.9979836940765381),\n",
       " ('deleted', 0.9978646039962769),\n",
       " ('way', 0.9977639317512512),\n",
       " ('going', 0.9977407455444336),\n",
       " ('news', 0.9975132942199707),\n",
       " ('bitcoin', 0.9974022507667542),\n",
       " ('got', 0.9972648620605469),\n",
       " ('week', 0.997062087059021)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('doge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3213660)\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.train(comments, total_examples=word2vec.corpus_count, epochs=word2vec.epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fd5e0e68130>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03869801  0.01764409 -0.00234458 ... -0.05396579  0.0022748\n",
      "  -0.02506784]\n",
      " [ 0.03272341  0.01611841  0.00059652 ... -0.05530431  0.0008333\n",
      "  -0.02268457]\n",
      " [ 0.0324393   0.01325334  0.0021516  ... -0.05470357  0.00699584\n",
      "  -0.02178209]\n",
      " [ 0.03595109  0.01757328  0.00103821 ... -0.05560936  0.00473393\n",
      "  -0.02174076]\n",
      " [ 0.03675701  0.02153272 -0.00259163 ... -0.05126861  0.00753019\n",
      "  -0.02464241]]\n"
     ]
    }
   ],
   "source": [
    "class WordVecVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = 300\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in texts.split() if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for texts in X\n",
    "        ])\n",
    "\n",
    "wtv = WordVecVectorizer(word2vec_model)\n",
    "X_wtv = wtv.transform(comments)\n",
    "print(X_wtv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "<ipython-input-275-0b4962db2f99>:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  print(word2vec_model.wv.vocab.keys())\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_wtv = TruncatedSVD(topics)\n",
    "X_lsa_wtv = lsa_tfid.fit_transform(X_wtv)\n",
    "# lsa_wtv.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-276-9addc4b75dba>:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  display_topics(lsa_wtv, word2vec_model.wv.vocab.keys(), topic_words, topic_names)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TruncatedSVD' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-9addc4b75dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsa_wtv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Metis/git_repos/cryptocurrency-analysis/Notebooks/NLP_functions.py\u001b[0m in \u001b[0;36mdisplay_topics\u001b[0;34m(model, feature_names, no_top_words, topic_names)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtopic_names\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtopic_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTopic \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TruncatedSVD' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_wtv, word2vec_model.wv.vocab.keys(), topic_words, topic_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# km = KMeans(\n",
    "#     n_clusters=2, init='random',\n",
    "#     n_init=10, max_iter=300, \n",
    "#     tol=1e-04, random_state=0\n",
    "# )\n",
    "# y_km = km.fit_predict(X_wtv)\n",
    "# df = pd.DataFrame({'year' :comments, 'topic_cluster' :y_km })\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n",
    "#                         n_iter=1000, verbose=1, random_state=0, angle=0.75)\n",
    "# tsne_features = model.fit_transform(lda_matrix)\n",
    "# df = pd.DataFrame(tsne_features)\n",
    "# df['topic'] = lda_matrix.argmax(axis=1)\n",
    "# df.columns = ['TSNE1', 'TSNE2', 'topic']\n",
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.title('T-SNE plot of different headlines ( headlines are clustered among their topics)')\n",
    "# ax = sns.scatterplot(x = 'TSNE1', y = 'TSNE2', hue = 'topic', data = df, legend = 'full')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_top_n_words(corpus, n=10):\n",
    "#   vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "#   bag_of_words = vec.transform(corpus)\n",
    "#   sum_words = bag_of_words.sum(axis=0) \n",
    "#   words_freq = [(word, sum_words[0, idx]) for word, idx in   vec.vocabulary_.items()]\n",
    "#   words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "#   return words_freq[:n]\n",
    "# words = []\n",
    "# word_values = []\n",
    "# for i,j in get_top_n_words(headlines['headline_text'],15):\n",
    "#   words.append(i)\n",
    "#   word_values.append(j)\n",
    "# fig, ax = plt.subplots(figsize=(16,8))\n",
    "# ax.bar(range(len(words)), word_values);\n",
    "# ax.set_xticks(range(len(words)));\n",
    "# ax.set_xticklabels(words, rotation='vertical');\n",
    "# ax.set_title('Top 15 words in the headlines dataset');\n",
    "# ax.set_xlabel('Word');\n",
    "# ax.set_ylabel('Number of occurences');\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# all_words = ''.join([word for word in headlines['headline_text'][0:100000]])\n",
    "# all_words\n",
    "# wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Some frequent words used in the headlines\", weight='bold', fontsize=14)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
