{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#api\n",
    "import praw\n",
    "\n",
    "#pre-processing\n",
    "import nltk \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "# import string\n",
    "#from textblob import TextBlob\n",
    "#from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n",
    "# nltk.download()\n",
    "\n",
    "from NLP_functions import display_topics, prob_df, topic_matrix\n",
    "\n",
    "#analysis\n",
    "from sklearn.decomposition import TruncatedSVD, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.1.4 was released Sunday February 07, 2021.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='oCxykEUclueAuA', client_secret='M8wVQw3nFxMsXHLGxurnTtkN-Mdvtg', user_agent='Crypto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top posts of all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  score      id  \\\n",
      "0    Elon Musk reminds me of a rich billionaire who...  39195  lce6zm   \n",
      "1    CryptoNick is deleting all of his BitConnect v...  26505  7r0ftz   \n",
      "2    Reminder: Robinhood blocked several stocks fro...  26228  lfrslo   \n",
      "3    I will tell you exactly what is going on here,...  20146  7vga1y   \n",
      "4    Robinhood is launching a Crypto Trading app to...  19966  7sx5ze   \n",
      "..                                                 ...    ...     ...   \n",
      "927  Apple Co-Founder Steve Wozniak Outlines the Pa...   1491  9rcezl   \n",
      "928  PwC Hong Kong and PwC Singapore announce joint...   1489  8gy2l6   \n",
      "929                                 We Are All Satoshi   1486  gqt4ze   \n",
      "930  Hackers Donate Stolen Bitcoin Worth $20,000 to...   1487  jeowcu   \n",
      "931             Top 101 Coins Grouped by Usage/Purpose   1486  lgeots   \n",
      "\n",
      "          subreddit                                                url  \\\n",
      "0    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "1    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "2    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "3    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "4    CryptoCurrency  http://blog.robinhood.com/news/2018/1/24/dont-...   \n",
      "..              ...                                                ...   \n",
      "927  CryptoCurrency  https://dailyhodl.com/2018/10/25/apple-co-foun...   \n",
      "928  CryptoCurrency  https://www.pwchk.com/en/press-room/press-rele...   \n",
      "929  CryptoCurrency                https://i.redd.it/bsykace1l2151.gif   \n",
      "930  CryptoCurrency  https://cryptopotato.com/hackers-donate-stolen...   \n",
      "931  CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "\n",
      "     num_comments                                               body  \\\n",
      "0            3627  Anyone kinda disturbed by his behaviour? He cl...   \n",
      "1            3027  We can't let these legendary affiliate scammer...   \n",
      "2            1098  Stop fucking around with these corporate hacks...   \n",
      "3            1431  /edit: Hi /r/all. While I have your attention,...   \n",
      "4            3857                                                      \n",
      "..            ...                                                ...   \n",
      "927           108                                                      \n",
      "928           337                                                      \n",
      "929            90                                                      \n",
      "930           155                                                      \n",
      "931           247  I was sort of shocked I couldn't find this dat...   \n",
      "\n",
      "                   date  \n",
      "0   2021-02-04 12:02:16  \n",
      "1   2018-01-17 11:46:08  \n",
      "2   2021-02-09 01:55:15  \n",
      "3   2018-02-05 17:21:09  \n",
      "4   2018-01-25 16:19:04  \n",
      "..                  ...  \n",
      "927 2018-10-25 17:31:26  \n",
      "928 2018-05-04 09:55:54  \n",
      "929 2020-05-26 08:28:25  \n",
      "930 2020-10-20 12:54:38  \n",
      "931 2021-02-09 22:02:05  \n",
      "\n",
      "[932 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('Cryptocurrency')\n",
    "\n",
    "for post in list(ml_subreddit.top('all', limit=1000)):\n",
    "    date = datetime.datetime.utcfromtimestamp(post.created_utc)\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, date])\n",
    "    \n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'date'])\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top coments from each top post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               comment                date\n",
      "0     If your life savings hinges on a Tweet from a... 2021-02-04 12:02:16\n",
      "1     **Some Important Reminders:**\\n\\n* Even in th... 2018-01-17 11:46:08\n",
      "2     Itâ€™s not even real Bitcoin.  You can never wi... 2021-02-09 01:55:15\n",
      "3     [deleted] It's a shame i can vote this up onl... 2018-02-05 17:21:09\n",
      "4     Reminder: all referral links are prohibited i... 2018-01-25 16:19:04\n",
      "..                                                 ...                 ...\n",
      "927   Not a bad supporter to have! Show me your bag... 2018-10-25 17:31:26\n",
      "928   Alibaba working with PWC on a food supply cha... 2018-05-04 09:55:54\n",
      "929   \"I am the real Slim Shady\" lol Except Craig W... 2020-05-26 08:28:25\n",
      "930   Who is the true Robinhood here? > Bitcoin Wor... 2020-10-20 12:54:38\n",
      "931   Well written! Didnt know there was soo many s... 2021-02-09 22:02:05\n",
      "\n",
      "[932 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "top_comments = []\n",
    "for i,d in zip(posts.id, posts.date):\n",
    "    try:\n",
    "        submission = reddit.submission(id=i)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        comments=list([(comment.body) for comment in submission.comments.list()])\n",
    "        comment_string = ''\n",
    "        for x in comments[:20]:\n",
    "            comment_string = comment_string + ' ' + x\n",
    "        top_comments.append([comment_string,d])\n",
    "    except:\n",
    "        comments=None\n",
    "top_comments = pd.DataFrame(top_comments,columns=['comment', 'date'])\n",
    "print(top_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine comments by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    404\n",
       "2020    200\n",
       "2021    125\n",
       "2019    114\n",
       "2017     89\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_comments['year'] = top_comments['date'].dt.year\n",
    "years = ['2017', '2018', '2019', '2020', '2021']\n",
    "top_comments['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_year = pd.DataFrame(index=years, columns=['Comments'])\n",
    "for y in years:\n",
    "    comment_string = ''\n",
    "    for index, row in top_comments.iterrows():\n",
    "        if str(row[2]) == y:\n",
    "            comment_string = comment_string + ' ' + row[0]\n",
    "            \n",
    "    comment_string = re.sub(r'\\([^)]*\\)', '', comment_string) #remove links\n",
    "    comment_string = re.sub('\\w*\\d\\w*', '', comment_string) #remove numbers\n",
    "    comment_string = re.sub(r'[^\\w\\s]', '', (comment_string))#remove punctuation\n",
    "    comment_string = comment_string.lower() #lowercase\n",
    "    comment_string = comment_string.replace('btc', 'bitcoin') #replace bitcoin acronym \n",
    "    \n",
    "    #create stop words list\n",
    "    stop = stopwords.words('english')\n",
    "    stop += [ 'moderator', 'subreddit', 'reddit' , 'question', 'concern', 'please', 'contact', 'im', \n",
    "            'people', 'gon', 'seems', 'something', 'year', 'like', 'mod']\n",
    "    stop = set(stop)\n",
    "\n",
    "    mwe_tokenizer = MWETokenizer([ ('bull','run'), ('market','cap'), ('pump','dump'),\n",
    "                             ('stock','market'), ('flaired', 'inaccurately')])\n",
    "    \n",
    "    comment_token = mwe_tokenizer.tokenize(word_tokenize(comment_string))  # tokenize words\n",
    "\n",
    "    #lemmatize words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    comment_token_lemm = [(lemmatizer.lemmatize(w)) for w in comment_token]\n",
    "\n",
    "    comment_token_lemm = [w for w in comment_token_lemm if w not in stop] #remove stop words\n",
    "    \n",
    "    comment_token_lemm = \" \".join(comment_token_lemm) #join back into one string\n",
    "    \n",
    "    comment_year.loc[y, 'Comments'] = comment_token_lemm #add to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comments\n",
      "2017  longer version raccoon loses two piece candyfl...\n",
      "2018  important reminder even case public figure str...\n",
      "2019  twitter post random guy popular wa giving legi...\n",
      "2020  offered cabinet position biden admin tldr form...\n",
      "2021  life saving hinge tweet crazy billionaire dont...\n"
     ]
    }
   ],
   "source": [
    "print(comment_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment_year = re.sub(r'\\([^)]*\\)', '', all_comments) #remove links\n",
    "# comment_year = re.sub('\\w*\\d\\w*', '', all_comments) #remove numbers\n",
    "# comment_year = re.sub(r'[^\\w\\s]', '', (all_comments))#remove punctuation\n",
    "# comment_year = comment_year.lower() #lowercase\n",
    "# comment_year = comment_year.replace('btc', 'bitcoin') #replace bitcoin acronym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_comments = comment_year.loc['2017', 'Comments'] #concatenate comments column into one string\n",
    "\n",
    "# all_comments = re.sub(r'\\([^)]*\\)', '', all_comments) #remove links\n",
    "# all_comments = re.sub('\\w*\\d\\w*', '', all_comments) #remove numbers\n",
    "# all_comments = re.sub(r'[^\\w\\s]', '', (all_comments))#remove punctuation\n",
    "# all_comments = all_comments.lower() #lowercase\n",
    "# all_comments = all_comments.replace('btc', 'bitcoin') #replace bitcoin acronym \n",
    "\n",
    "# #create stop words list\n",
    "# stop = stopwords.words('english')\n",
    "# stop += [ 'moderator', 'subreddit', 'reddit' , 'question', 'concern', 'please', 'contact', 'im', \n",
    "#         'people', 'gon', 'seems', 'something', 'year', 'like', 'mod']\n",
    "# stop = set(stop)\n",
    "\n",
    "# mwe_tokenizer = MWETokenizer([ ('bull','run'), ('market','cap'), ('pump','dump'),\n",
    "#                              ('stock','market'), ('flaired', 'inaccurately'), ('get', 'rich', 'quick')])\n",
    "\n",
    "# words = mwe_tokenizer.tokenize(word_tokenize(all_comments))  # tokenize words\n",
    "\n",
    "# #lemmatize words\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# words = [(lemmatizer.lemmatize(w)) for w in words]\n",
    "\n",
    "# words = [w for w in words if w not in stop] #remove stop words\n",
    "\n",
    "# #create n grams\n",
    "# n = 2\n",
    "# bigrams = ngrams(words, n)\n",
    "\n",
    "# #count phrases\n",
    "# counter = Counter()\n",
    "# counter += Counter(bigrams)\n",
    "\n",
    "# for word, count in counter.most_common(30):\n",
    "#     print('%20s %i' % (\" \".join(word), count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = s.apply(fun) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(comment_year['Comments'])\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop)\n",
    "X= cv.fit_transform(comments)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "\n",
    "topics = 3\n",
    "topic_words = 10\n",
    "topic_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis (LSA) with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91733209, 0.03995111, 0.0229522 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_cv = TruncatedSVD(topics)\n",
    "X_lsa_cv = lsa_cv.fit_transform(X)\n",
    "lsa_cv.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f0ceab12c7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsa_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# topic_word = pd.DataFrame(lsa.components_.round(3),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#              index = [\"component_1\",\"component_2\", \"component_3\"],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#              columns = cv.get_feature_names())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# topic_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Metis/git_repos/cryptocurrency-analysis/Notebooks/NLP_functions.py\u001b[0m in \u001b[0;36mtopic_matrix\u001b[0;34m(model, feature_names, topics)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topic_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     topic_word = pd.DataFrame(model.components_.round(3),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "print(topic_matrix(lsa_cv, cv.get_feature_names(), topics))\n",
    "# topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "#              index = [\"component_1\",\"component_2\", \"component_3\"],\n",
    "#              columns = cv.get_feature_names())\n",
    "# topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "#     for ix, topic in enumerate(model.components_):\n",
    "#         if not topic_names or not topic_names[ix]:\n",
    "#             print(\"\\nTopic \", ix)\n",
    "#         else:\n",
    "#             print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "#         print(\", \".join([feature_names[i]\n",
    "#                         for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_cv, cv.get_feature_names(), topic_words, topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df(X_lsa_cv, topics)\n",
    "# Vt = pd.DataFrame(doc_topic.round(5),\n",
    "#              index = ['2017', '2018', '2019', '2020', '2021']s,\n",
    "#              columns = [\"component_1\",\"component_2\", \"component_3\" ])\n",
    "# Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(3)\n",
    "X_topic = nmf_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\"],\n",
    "             columns = cv.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_model, cv.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pd.DataFrame(doc_topic.round(5),\n",
    "             index = comments,\n",
    "             columns = [\"component_1\",\"component_2\", \"component_3\" ])\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-ID Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
    "doc_vectors = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-68dcfb427789>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-68dcfb427789>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    jupyter kernelspec list\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
