{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#api\n",
    "import praw\n",
    "\n",
    "#pre-processing\n",
    "import nltk \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "# import string\n",
    "#from textblob import TextBlob\n",
    "#from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "# nltk.download()\n",
    "\n",
    "#analysis\n",
    "from sklearn.decomposition import TruncatedSVD, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.1.4 was released Sunday February 07, 2021.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='oCxykEUclueAuA', client_secret='M8wVQw3nFxMsXHLGxurnTtkN-Mdvtg', user_agent='Crypto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top posts of all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  score      id  \\\n",
      "0    Elon Musk reminds me of a rich billionaire who...  39166  lce6zm   \n",
      "1    CryptoNick is deleting all of his BitConnect v...  26507  7r0ftz   \n",
      "2    Reminder: Robinhood blocked several stocks fro...  26220  lfrslo   \n",
      "3    I will tell you exactly what is going on here,...  20143  7vga1y   \n",
      "4    Robinhood is launching a Crypto Trading app to...  19969  7sx5ze   \n",
      "..                                                 ...    ...     ...   \n",
      "927                                 We Are All Satoshi   1487  gqt4ze   \n",
      "928  Hackers Donate Stolen Bitcoin Worth $20,000 to...   1491  jeowcu   \n",
      "929  Venezuelan money so worthless they just use it...   1485  bl34bp   \n",
      "930  Your Guide to Monero, and Why It Has Great Pot...   1488  7ra409   \n",
      "931             Top 101 Coins Grouped by Usage/Purpose   1484  lgeots   \n",
      "\n",
      "          subreddit                                                url  \\\n",
      "0    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "1    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "2    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "3    CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "4    CryptoCurrency  http://blog.robinhood.com/news/2018/1/24/dont-...   \n",
      "..              ...                                                ...   \n",
      "927  CryptoCurrency                https://i.redd.it/bsykace1l2151.gif   \n",
      "928  CryptoCurrency  https://cryptopotato.com/hackers-donate-stolen...   \n",
      "929  CryptoCurrency                https://i.redd.it/clzw2tmj0fw21.jpg   \n",
      "930  CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "931  CryptoCurrency  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "\n",
      "     num_comments                                               body  \\\n",
      "0            3627  Anyone kinda disturbed by his behaviour? He cl...   \n",
      "1            3027  We can't let these legendary affiliate scammer...   \n",
      "2            1097  Stop fucking around with these corporate hacks...   \n",
      "3            1431  /edit: Hi /r/all. While I have your attention,...   \n",
      "4            3857                                                      \n",
      "..            ...                                                ...   \n",
      "927            90                                                      \n",
      "928           155                                                      \n",
      "929           218                                                      \n",
      "930           381  #**/////Your Guide to Monero, and Why It Has G...   \n",
      "931           247  I was sort of shocked I couldn't find this dat...   \n",
      "\n",
      "                   date  \n",
      "0   2021-02-04 12:02:16  \n",
      "1   2018-01-17 11:46:08  \n",
      "2   2021-02-09 01:55:15  \n",
      "3   2018-02-05 17:21:09  \n",
      "4   2018-01-25 16:19:04  \n",
      "..                  ...  \n",
      "927 2020-05-26 08:28:25  \n",
      "928 2020-10-20 12:54:38  \n",
      "929 2019-05-05 21:12:56  \n",
      "930 2018-01-18 14:42:02  \n",
      "931 2021-02-09 22:02:05  \n",
      "\n",
      "[932 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('Cryptocurrency')\n",
    "\n",
    "for post in list(ml_subreddit.top('all', limit=1000)):\n",
    "    date = datetime.datetime.utcfromtimestamp(post.created_utc)\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, date])\n",
    "    \n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'date'])\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top coments from each top post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment                date\n",
      "0      If your life savings hinges on a Tweet from a ... 2021-02-04 12:02:16\n",
      "1      I seriously don’t understand the obsession wit... 2021-02-04 12:02:16\n",
      "2      Tbh u can't place all the blame on Elon tho, t... 2021-02-04 12:02:16\n",
      "3      seriously, there's so much more relevant peopl... 2021-02-04 12:02:16\n",
      "4      If I'm Elon, I put hundreds of millions into D... 2021-02-04 12:02:16\n",
      "...                                                  ...                 ...\n",
      "18616  What kept you from keeping Monero from digital... 2021-02-09 22:02:05\n",
      "18617             Wow that‘s a good overview. Thank you. 2021-02-09 22:02:05\n",
      "18618  Hi guys,\\n\\nI am from India and there is a pos... 2021-02-09 22:02:05\n",
      "18619  I use nano as a store of value is it not a sto... 2021-02-09 22:02:05\n",
      "18620  Vechain is not a supply chain logistics compan... 2021-02-09 22:02:05\n",
      "\n",
      "[18621 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "top_comments = []\n",
    "for i,d in zip(posts.id, posts.date):\n",
    "    try:\n",
    "        submission = reddit.submission(id=i)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        comments=list([(comment.body) for comment in submission.comments.list()])\n",
    "        for x in comments[:20]:\n",
    "            top_comments.append([x,d])\n",
    "    except:\n",
    "        comments=None\n",
    "top_comments = pd.DataFrame(top_comments,columns=['comment', 'date'])\n",
    "print(top_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bot action wa 104\n",
      " action wa performed 104\n",
      "wa performed automatically 104\n",
      "flaired_inaccurately click bot 68\n",
      "    click bot action 68\n",
      "submission wa flaired_inaccurately 65\n",
      "wa flaired_inaccurately click 65\n",
      "summary auto generated 24\n",
      "abstract history exchange 20\n",
      "history exchange wallet 20\n",
      "exchange wallet bias 20\n",
      "wallet bias argument 20\n",
      "bias argument argument 20\n",
      "argument argument cryptowikis 20\n",
      "argument cryptowikis policy 20\n",
      "cryptowikis policy contribute 20\n",
      "  auto generated bot 20\n",
      " generated bot meant 20\n",
      "   bot meant replace 20\n",
      "meant replace reading 20\n",
      "replace reading original 20\n",
      "reading original article 20\n",
      "original article always 20\n",
      " article always dyor 20\n",
      "  basic info website 19\n",
      "      get rich quick 17\n",
      "      dont even know 17\n",
      "      word word word 17\n",
      "policy contribute content 16\n",
      "contribute content bot 16\n"
     ]
    }
   ],
   "source": [
    "all_comments = ' '.join(top_comments['comment']) #concatenate comments column into one string\n",
    "\n",
    "all_comments = re.sub(r'\\([^)]*\\)', '', all_comments) #remove links\n",
    "all_comments = re.sub('\\w*\\d\\w*', '', all_comments) #remove numbers\n",
    "all_comments = re.sub(r'[^\\w\\s]', '', (all_comments))#remove punctuation\n",
    "all_comments = all_comments.lower() #lowercase\n",
    "all_comments = all_comments.replace('btc', 'bitcoin') #replace bitcoin acronym \n",
    "\n",
    "#create stop words list\n",
    "stop = stopwords.words('english')\n",
    "stop += ['crypto', 'moderator', 'subreddit', 'reddit' , 'question', 'concern', 'please', 'contact', 'im', \n",
    "        'people', 'gon', 'seems', 'something', 'year', 'like']\n",
    "stop = set(stop)\n",
    "\n",
    "mwe_tokenizer = MWETokenizer([('doge','coin'), ('bull','run'), ('market','cap'), ('pump','dump'),\n",
    "                             ('stock','market'), ('flaired', 'inaccurately')])\n",
    "\n",
    "words = mwe_tokenizer.tokenize(word_tokenize(all_comments))  # tokenize words\n",
    "\n",
    "#lemmatize words\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = [(lemmatizer.lemmatize(w)) for w in words]\n",
    "\n",
    "words = [w for w in words if w not in stop] #remove stop words\n",
    "\n",
    "#create n grams\n",
    "n = 3\n",
    "bigrams = ngrams(words, n)\n",
    "\n",
    "#count phrases\n",
    "counter = Counter()\n",
    "counter += Counter(bigrams)\n",
    "\n",
    "for word, count in counter.most_common(30):\n",
    "    print('%20s %i' % (\" \".join(word), count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_comments = ' '.join(top_comments['comment']) #concatenate comments column into one string\n",
    "\n",
    "# all_comments = all_comments.lower() #lowercase\n",
    "# all_comments = all_comments.replace('btc', 'bitcoin') #replace bitcoin acronym \n",
    "# all_comments = re.sub(r'\\([^)]*\\)', '', all_comments) #remove links\n",
    "\n",
    "# mwe_tokenizer = MWETokenizer([('doge','coin')])\n",
    "\n",
    "# words = mwe_tokenize.tokenize(TextBlob(all_comments).words)  # tokenize words\n",
    "\n",
    "\n",
    "# words = [(re.sub('\\w*\\d\\w*', '', w)) for w in words] #remove numbers\n",
    "# words = [(re.sub(r'[^\\w\\s]', '', (w))) for w in words] #remove punctuation\n",
    "\n",
    "# #create stop words list\n",
    "# stop = stopwords.words('english')\n",
    "# stop += ['crypto', 'moderator', 'subreddit', 'reddit' , 'question', 'concern', 'please', 'contact', 'im', ' ']\n",
    "# stop = set(stop)\n",
    "\n",
    "# #lemmatize words\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# words = [(lemmatizer.lemmatize(w)) for w in words]\n",
    "\n",
    "# words = [w for w in words if w not in stop] #remove stop words\n",
    "\n",
    "# #create n grams\n",
    "# n = 2\n",
    "# bigrams = ngrams(words, n)\n",
    "\n",
    "# #count phrases\n",
    "# counter = Counter()\n",
    "# counter += Counter(bigrams)\n",
    "\n",
    "# for word, count in counter.most_common(30):\n",
    "#     print('%20s %i' % (\" \".join(word), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-02f4a77addb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set(stopwords.words('english'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comments_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "comments = list(top_comments['comment'])\n",
    "\n",
    "#comments_cleaned = [re.sub('\\w*\\d\\w*', '', i) for i in comments]\n",
    "\n",
    "# set(stopwords.words('english'))\n",
    "cv = CountVectorizer(stop_words=stop)\n",
    "X= cv.fit_transform(comments_cleaned)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(3)\n",
    "doc_topic = lsa.fit_transform(X)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\"],\n",
    "             columns = cv.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa, cv.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vt = pd.DataFrame(doc_topic.round(5),\n",
    "             index = comments,\n",
    "             columns = [\"component_1\",\"component_2\", \"component_3\" ])\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(3)\n",
    "doc_topic = nmf_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\"],\n",
    "             columns = cv.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_model, cv.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pd.DataFrame(doc_topic.round(5),\n",
    "             index = comments,\n",
    "             columns = [\"component_1\",\"component_2\", \"component_3\" ])\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
