{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NLP_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c9a29860c882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# nltk.download()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNLP_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NLP_functions'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#api\n",
    "import praw\n",
    "\n",
    "#pre-processing\n",
    "import nltk \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "# import string\n",
    "#from textblob import TextBlob\n",
    "#from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "# nltk.download()\n",
    "\n",
    "from NLP_functions.py import display_topics, prob_df, topic_matrix\n",
    "\n",
    "#analysis\n",
    "from sklearn.decomposition import TruncatedSVD, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='oCxykEUclueAuA', client_secret='M8wVQw3nFxMsXHLGxurnTtkN-Mdvtg', user_agent='Crypto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top posts of all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('Cryptocurrency')\n",
    "\n",
    "for post in list(ml_subreddit.top('all', limit=1000)):\n",
    "    date = datetime.datetime.utcfromtimestamp(post.created_utc)\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, date])\n",
    "    \n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'date'])\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top coments from each top post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_comments = []\n",
    "for i,d in zip(posts.id, posts.date):\n",
    "    try:\n",
    "        submission = reddit.submission(id=i)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        comments=list([(comment.body) for comment in submission.comments.list()])\n",
    "        comment_string = ''\n",
    "        for x in comments[:20]:\n",
    "            comment_string = comment_string + ' ' + x\n",
    "        top_comments.append([comment_string,d])\n",
    "    except:\n",
    "        comments=None\n",
    "top_comments = pd.DataFrame(top_comments,columns=['comment', 'date'])\n",
    "print(top_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine comments by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['year'] = top_comments['date'].dt.year\n",
    "years = ['2017', '2018', '2019', '2020', '2021']\n",
    "top_comments['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_year = pd.DataFrame(index=years, columns=['Comments'])\n",
    "for y in years:\n",
    "    comment_string = ''\n",
    "    for index, row in top_comments.iterrows():\n",
    "        if str(row[2]) == y:\n",
    "            comment_string = comment_string + ' ' + row[0]\n",
    "            \n",
    "    comment_string = re.sub(r'\\([^)]*\\)', '', comment_string) #remove links\n",
    "    comment_string = re.sub('\\w*\\d\\w*', '', comment_string) #remove numbers\n",
    "    comment_string = re.sub(r'[^\\w\\s]', '', (comment_string))#remove punctuation\n",
    "    comment_string = comment_string.lower() #lowercase\n",
    "    comment_string = comment_string.replace('btc', 'bitcoin') #replace bitcoin acronym \n",
    "    \n",
    "    #create stop words list\n",
    "    stop = stopwords.words('english')\n",
    "    stop += [ 'moderator', 'subreddit', 'reddit' , 'question', 'concern', 'please', 'contact', 'im', \n",
    "            'people', 'gon', 'seems', 'something', 'year', 'like', 'mod']\n",
    "    stop = set(stop)\n",
    "\n",
    "    mwe_tokenizer = MWETokenizer([ ('bull','run'), ('market','cap'), ('pump','dump'),\n",
    "                             ('stock','market'), ('flaired', 'inaccurately'), ('get', 'rich', 'quick')])\n",
    "    \n",
    "    comment_token = mwe_tokenizer.tokenize(word_tokenize(comment_string))  # tokenize words\n",
    "\n",
    "    #lemmatize words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    comment_token_lemm = [(lemmatizer.lemmatize(w)) for w in comment_token]\n",
    "\n",
    "    comment_token_lemm = [w for w in comment_token_lemm if w not in stop] #remove stop words\n",
    "    \n",
    "    comment_token_lemm = \" \".join(comment_token_lemm)\n",
    "    \n",
    "    comment_year.loc[y, 'Comments'] = comment_token_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comment_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_year = re.sub(r'\\([^)]*\\)', '', all_comments) #remove links\n",
    "comment_year = re.sub('\\w*\\d\\w*', '', all_comments) #remove numbers\n",
    "comment_year = re.sub(r'[^\\w\\s]', '', (all_comments))#remove punctuation\n",
    "comment_year = comment_year.lower() #lowercase\n",
    "comment_year = comment_year.replace('btc', 'bitcoin') #replace bitcoin acronym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = comment_year.loc['2017', 'Comments'] #concatenate comments column into one string\n",
    "\n",
    "all_comments = re.sub(r'\\([^)]*\\)', '', all_comments) #remove links\n",
    "all_comments = re.sub('\\w*\\d\\w*', '', all_comments) #remove numbers\n",
    "all_comments = re.sub(r'[^\\w\\s]', '', (all_comments))#remove punctuation\n",
    "all_comments = all_comments.lower() #lowercase\n",
    "all_comments = all_comments.replace('btc', 'bitcoin') #replace bitcoin acronym \n",
    "\n",
    "#create stop words list\n",
    "stop = stopwords.words('english')\n",
    "stop += [ 'moderator', 'subreddit', 'reddit' , 'question', 'concern', 'please', 'contact', 'im', \n",
    "        'people', 'gon', 'seems', 'something', 'year', 'like', 'mod']\n",
    "stop = set(stop)\n",
    "\n",
    "mwe_tokenizer = MWETokenizer([ ('bull','run'), ('market','cap'), ('pump','dump'),\n",
    "                             ('stock','market'), ('flaired', 'inaccurately'), ('get', 'rich', 'quick')])\n",
    "\n",
    "words = mwe_tokenizer.tokenize(word_tokenize(all_comments))  # tokenize words\n",
    "\n",
    "#lemmatize words\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = [(lemmatizer.lemmatize(w)) for w in words]\n",
    "\n",
    "words = [w for w in words if w not in stop] #remove stop words\n",
    "\n",
    "#create n grams\n",
    "n = 2\n",
    "bigrams = ngrams(words, n)\n",
    "\n",
    "#count phrases\n",
    "counter = Counter()\n",
    "counter += Counter(bigrams)\n",
    "\n",
    "for word, count in counter.most_common(30):\n",
    "    print('%20s %i' % (\" \".join(word), count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = s.apply(fun) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(comment_year['Comments'])\n",
    "\n",
    "#comments_cleaned = [re.sub('\\w*\\d\\w*', '', i) for i in comments]\n",
    "\n",
    "# set(stopwords.words('english'))\n",
    "cv = CountVectorizer(stop_words=stop)\n",
    "X= cv.fit_transform(comments)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis (LSA) with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(3)\n",
    "doc_topic = lsa.fit_transform(X)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\"],\n",
    "             columns = cv.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa, cv.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vt = pd.DataFrame(doc_topic.round(5),\n",
    "             index = ['2017', '2018', '2019', '2020', '2021']s,\n",
    "             columns = [\"component_1\",\"component_2\", \"component_3\" ])\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(3)\n",
    "X_topic = nmf_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\"],\n",
    "             columns = cv.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_model, cv.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = pd.DataFrame(doc_topic.round(5),\n",
    "             index = comments,\n",
    "             columns = [\"component_1\",\"component_2\", \"component_3\" ])\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
